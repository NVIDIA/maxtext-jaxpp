# model config for grok proxy (16 layers)

base_emb_dim: 6144
base_num_query_heads: 48
base_num_kv_heads: 8
base_mlp_dim: 14336
base_num_decoder_layers: 16
head_dim: 128
mlp_activations: ["gelu"]
vocab_size: 131072
enable_dropout: False
logits_via_embedding: True
normalization_layer_epsilon: 1.0e-5
num_experts: 8
num_experts_per_tok: 2
widening_factor: 8.0
capacity_factor: 2.0
decoder_block: "grok"
attention: "jaxpp_te"
logical_axis_rules: [
                      ['activation_batch', ['data', 'fsdp', 'fsdp_transpose', 'expert']],
                      ['activation_batch_no_exp', ['data', 'fsdp', 'fsdp_transpose']],
                      ['activation_exp', 'expert'],
                      ['activation_group', ['data', 'fsdp', 'fsdp_transpose', 'expert']],
                      ['activation_group', ['data', 'fsdp', 'fsdp_transpose']],
                       # For pipeline parallelism the pre and post decoder layer tensors' batch dimension is sharded by stages.
                       # Microbatches are sharded by stage, so moving out of and into this sharding should be a local reshape.
                       # The "stage" needs to be listed first since the microbatch dimension is first before the reshape.
                      ['activation_embed_and_logits_batch', ['stage', 'data', 'fsdp', 'fsdp_transpose', 'expert']],
                      ['activation_heads', ['tensor', 'sequence', 'expert']],
                      ['activation_kv_heads', ['tensor','sequence']],
                      ['activation_length', 'sequence'],
                      ['activation_embed', ['tensor', 'expert']],
                      ['activation_embed', 'tensor'],
                      ['activation_mlp', 'tensor'],
                      ['activation_kv', 'tensor'],
                      ['activation_kv_batch', ['data', 'fsdp', 'fsdp_transpose', 'expert']],
                      ['activation_kv_head_dim', 'tensor'],
                      ['activation_vocab', ['tensor', 'expert']],
                      ['activation_vocab', ['tensor', 'sequence']],
                      ['activation_vocab', 'tensor'],
                      ['activation_vocab', 'sequence'],
                      ['activation_stage', 'stage'],
                      ['exp', 'expert'],
                      ['mlp', ['fsdp_transpose', 'tensor', 'autoregressive', 'expert']],
                      ['mlp', ['fsdp_transpose', 'tensor', 'autoregressive']],
                      ['vocab', ['tensor', 'autoregressive']],
                      ['embed', ['fsdp', 'fsdp_transpose', 'sequence', 'expert']],
                      ['embed', ['fsdp', 'sequence']],
                      ['embed_no_exp', ['fsdp', 'fsdp_transpose', 'sequence']],
                      ['embed_no_exp', ['fsdp', 'sequence']],
                      ['norm', ['tensor', 'expert']],
                      ['norm', 'tensor'],
                      ['heads', ['tensor', 'autoregressive']],
                      ['layers', 'stage'],
                      ['kv', []],
                      ['kv_heads', ['tensor', 'autoregressive']],
                      ['kv_head_dim', []],
                      ['cache_batch', []],
                      ['cache_heads', ['autoregressive', 'tensor']],
                      ['cache_kv', []],
                      ['cache_sequence', []],
                    ]
